---
title: "Test Math Rendering in Blog Posts with the Schur Complement"
date: 2025-04-08
permalink: /posts/2025/04/08/post1/
tags:
  - optimization
  - math
  - control
math: true
---

I have two main objectives in this post: 
- to leave a short note about the Schur complement, which I've been using quite a few times lately, and
- to test (by doing) the rendering of mathematical expressions in this blog post (.md file with MathJax). 

<!--more-->

The [**Schur complement**](https://en.wikipedia.org/wiki/Schur_complement) is a useful tool in modern contrl theory, and similarly in optimizations (e.g., [Semidefinite Programming](https://en.wikipedia.org/wiki/Semidefinite_programming)) because of its applicability in manipulating matrix-valued inequalities (e.g., [Linear Matrix Ineqaulities (LMI)](https://en.wikipedia.org/wiki/Linear_matrix_inequality)).

The Schur complement can be defined by the following statement.

Given a symmetric matrix \\( X = X^{\top} \\) (\\(X^{\top}\\) as a transpose of \\(X\\)):

$$
X = \begin{bmatrix} U & V \\ V^\top & W \end{bmatrix},
$$

where \\( U \\) is invertible (that is, \\(U^{-1}\\) exists), the **Schur complement** of \\( U \\) in \\( X \\) is defined as:

$$
S := W - V^{\top} U^{-1} V
$$

As you can see, it enables relating a block matrix \\( X \\) and its component blocks into a single matrix expression \\( S \\), the Schur complement.

Let us adopt the notation \\(G \succ 0\\) (respectively, \\(G \prec 0\\)) to mean that a matrix \\( G \\) is [positive definite](https://en.wikipedia.org/wiki/Definite_matrix) (respectively, negative definite).

The following statement is very useful in applying the Schur complement:

Let $$ X = \begin{bmatrix} U & V \\ V^{\top} & W \end{bmatrix} $$, and \\( U \\) is invertible. 

Then, \\( X  \succ 0 \\) if and only if both \\( U \succ 0 \\) and \\( S = W - V^{\top}U^{-1}V \succ 0 \\).


**<em>Example</em>**

We can illustrate the use of the statement above with a linear matrix inequality (LMI) that appears in the Kalman-Popov-Yakubovich (KYP) Lemma (also called [the Bounded real Lemma](https://en.wikibooks.org/w/index.php?title=LMIs_in_Control/pages/Discrete_Time_Bounded_Real_Lemma&veaction=edit&section=4))---a key result useful, for example, in robust control theory.

Since our main objective is not to explain the KYP Lemma but to illustrate the use of the Schur complemet, we will not go further into what the KYP Lemma is, and how useful it is. Perhaps it can be a future post.

The following LMI appears in the (discrete-time) KYP Lemma:

$$
M_{0} := \begin{bmatrix}A^{\top}PA - P & A^{\top}PB & C^{\top}\\
(A^{\top}PB)^{\top} & B^{\top}PB - \gamma I & D^{\top}\\
C & D & -\gamma I\end{bmatrix} \prec 0,
$$

where \\( P = P^{\top} \succ 0 \\) is a symmetric matrix, \\(I\\) is an identity matrix of appropriate dimensions and \\( \gamma > 0\\) is a positive scalar quantity (often the metric for the performance of robust controllers).

Interestingly, \\(M_{0} \prec 0 \\) has another (more practical in a sense) representation as follows:

$$
M_{1} := \begin{bmatrix}-P^{-1} & A & B & 0\\
A^{\top} & -P & 0 & C^{\top} \\
B^{\top} & 0 & -\gamma I & D^{\top}\\
0 & C & D & -\gamma I\end{bmatrix} \prec 0,
$$

where \\( P = P^{\top} \succ 0 \\).

Is it true? Yes, based on the Schur complement.

How? Consider the decomposition of \\( M_{0} \\) as follows:

$$
M_{0} = \underbrace{\begin{bmatrix}A^{\top}PA & A^{\top}PB & 0\\
(A^{\top}PB)^{\top} & B^{\top}PB & 0\\
0 & 0 & 0\end{bmatrix}}_{=\begin{bmatrix}A & B & 0\end{bmatrix}^{\top}P\begin{bmatrix}A & B & 0\end{bmatrix}} + \underbrace{\begin{bmatrix}- P & 0 & C^{\top}\\
0 & - \gamma I & D^{\top}\\
C & D & -\gamma I\end{bmatrix}}_{ =: W}.
$$

Notice that this is the same as saying (since we have \\(M_{0} \prec 0 \\))

$$
\begin{align}
W + \begin{bmatrix}A & B & 0\end{bmatrix}^{\top}P\begin{bmatrix}A & B & 0\end{bmatrix} & \prec 0 \\
W - \begin{bmatrix}A & B & 0\end{bmatrix}^{\top}((\underbrace{-P^{-1}}_{ =: U})^{-1})\underbrace{\begin{bmatrix}A & B & 0\end{bmatrix}}_{ =: V} & \prec 0 \\
W - V^{\top}U^{-1}V & \prec 0 \\
\end{align}
$$

which implies, by the Schur complement, that (we know \\(U := -P^{-1} \prec 0\\) since \\(P \succ 0\\))

$$
\begin{bmatrix} U & V \\ V^\top & W \end{bmatrix} \prec 0,
$$

or more explicitly,

$$
\begin{bmatrix} -P^{-1} & A & B & 0 \\ A^{\top} & -P & 0 & C^{\top} \\ B^{\top} & 0 & -\gamma I & D^{\top} \\ 0 & C & D & \gamma I \end{bmatrix} \prec 0.
$$

**The Schur complement allowed us to connect two LMIs \\( M_{0} \prec 0\\) and \\( M_{1} \prec 0\\).**

### ***** Main Takeaways *****

- The Schur complement proves to be useful when working with linear matrix inequalities (LMIs), which appear quite often in modern optimizations and control theory.
- \\( X = [U, V ; V^{\top}, W] \succ 0 \\) if and only if \\( U \succ 0 \\) and \\( S = W - V^{\top}U^{-1}V \succ 0 \\).
- In-line mathematical expressions can be rendered with an expression (or a variable) enclosed by \\) on the right, and \\( on the left. 
- Other stand-alone mathematical expressions can be rendered by encapsulating an expression with '$$' left and right.